{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction and Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:                                                                                                                                     \n",
    "Watson Developer Cloud: a client library for Watson services.                                                                                                                        \n",
    "NLTK: leading platform for building Python programs to work with human language data.                                                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Watson Developer Cloud package: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: watson-developer-cloud in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\n",
      "Requirement not upgraded as not directly required: pyOpenSSL>=16.2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: autobahn>=0.10.9 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: python-dateutil>=2.5.3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: Twisted>=13.2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: requests<3.0,>=2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: service-identity>=17.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: cryptography>=1.9 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: six>=1.5.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: txaio>=2.10.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from autobahn>=0.10.9->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: hyperlink>=17.1.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: constantly>=15.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: zope.interface>=4.4.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: incremental>=16.10.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: Automat>=0.3.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pyasn1-modules in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pyasn1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: attrs in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: asn1crypto>=0.21.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: cffi>=1.7 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: setuptools in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from zope.interface>=4.4.2->Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pycparser in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cffi>=1.7->cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\n",
      "Requirement not upgraded as not directly required: six in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install IBM Cloud Object Storage Client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement not upgraded as not directly required: ibm-cos-sdk in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\n",
      "Requirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk)\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now restart the kernel by choosing Kernel > Restart. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import packages and libraries\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions\n",
    "    \n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import datetime\n",
    "from nltk import word_tokenize,sent_tokenize,ne_chunk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Add configurable items of the notebook below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add your service credentials from IBM Cloud for the Watson services\n",
    "You must create a Watson Natural Language Understanding service on IBM Cloud. Create a service for Natural Language Understanding (NLU). Insert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2018-03-23',\n",
    "    username=\"9d8eddab-da9f-454c-a1cc-a02ed8c2fe92\",\n",
    "    password=\"A4JHnZoZ1Wwt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add your service credentials for Object Storage\n",
    "You must create Object Storage service on IBM Cloud. To access data in a file in Object Storage, you need the Object Storage authentication credentials. Insert the Object Storage authentication credentials as credentials_1 in the following cell after removing the current contents in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_1 = {\n",
    "    'IBM_API_KEY_ID': 'ZX0yr-wb3zUL7KJjxTu8pd4oIwDZP_KnZdwlnso-OtGs',\n",
    "    'IAM_SERVICE_ID': 'iam-ServiceId-990cfd83-56cc-4023-8bd3-821fa4d7f280',\n",
    "    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'BUCKET': 'imageanddocumentclassification-donotdelete-pr-asgjxxebuspap7',\n",
    "    'FILE': 'config_legaldocs.txt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Global Variables\n",
    "Add global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText='form-doc-6.txt'\n",
    "ConfigFileName_Entity='config_entity_extract.txt'\n",
    "ConfigFileName_Classify= 'config_legaldocs.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Configure and download required NLTK packages\n",
    "Download the 'punkt' and 'averaged_perceptron_tagger' NLTK packages for POS tagging usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dsxuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persistence and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client('s3',\n",
    "                    ibm_api_key_id=credentials_1['IBM_API_KEY_ID'],\n",
    "                    ibm_service_instance_id=credentials_1['IAM_SERVICE_ID'],\n",
    "                    ibm_auth_endpoint=credentials_1['IBM_AUTH_ENDPOINT'],\n",
    "                    config=Config(signature_version='oauth'),\n",
    "                    endpoint_url=credentials_1['ENDPOINT'])\n",
    "\n",
    "def get_file(filename):\n",
    "    '''Retrieve file from Cloud Object Storage'''\n",
    "    fileobject = cos.get_object(Bucket=credentials_1['BUCKET'], Key=filename)['Body']\n",
    "    return fileobject\n",
    "\n",
    "def load_string(fileobject):\n",
    "    '''Load the file contents into a Python string'''\n",
    "    text = fileobject.read()\n",
    "    return text\n",
    "\n",
    "def put_file(filename, filecontents):\n",
    "    '''Write file to Cloud Object Storage'''\n",
    "    resp = cos.put_object(Bucket=credentials_1['BUCKET'], Key=filename, Body=filecontents)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Input Data\n",
    "Read the data file for entity extraction from Object Store                                                                                                                               \n",
    "Read the configuration file for augumented entity-value pairs from Object Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residential Lease Agreement\n",
      "\n",
      "ABC\n",
      "This Residential Lease Agreement is made between the Landlord\n",
      "\n",
      "and the Tenant W on this date\n",
      "\n",
      "The Landlord hereby agrees to rent the Premises to the Tenant and Tenant hereby agrees to rent the\n",
      "Premises from the Landlord. The Premises is described as follows:\n",
      "\n",
      "Street Address:\n",
      "\n",
      "Premises Description:\n",
      "\n",
      "1. TERM:\n",
      "The Lease term shall be as follows (choose one):\n",
      "\n",
      "[] Fixed term lease beginning on and ending on for a total period\n",
      "of months.\n",
      "\n",
      "[] Month to month lease beginning on\n",
      "2. RENT:\n",
      "\n",
      "The Tenant agrees to pay the Landlord an amount of $ per month as rent on or before\n",
      "the day of each month.\n",
      "\n",
      "If rent due is not paid on of before the day of the month, Tenant agrees to pay a late charge of $\n",
      "plus an additional late charge of $ per day until the rent is paid in\n",
      "\n",
      "full.\n",
      "3. SECURITY DEPOSIT:\n",
      "\n",
      "The Tenant shall deposit an amount of $ to be held by the Landlord as security\n",
      "deposit. This deposit shall be refunded to the Tenant upon termination of this Lease after deducting for\n",
      "any of the following: default of rent payment, loss or damage to the Premises or its furnishings, any\n",
      "required cleaning of the Premises and for any other reason allowed by law.\n",
      "\n",
      "4. USE OF PREMISES:\n",
      "\n",
      "The Premises shall be occupied only by the Tenant and the following occupants:\n",
      "\n",
      "The Tenant shall use the Premises for residential purposes only and may not use it for any other purpose\n",
      "with the written consent of the Landlord. The Tenant may not sublet this Premises or assign this Lease to\n",
      "any other persons without the written consent of the Landlord.\n"
     ]
    }
   ],
   "source": [
    "text_file= load_string(get_file(sampleText))\n",
    "if isinstance(text_file, bytes):\n",
    "    text_file = text_file.decode('utf-8') \n",
    "print(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "\t\"configuration\":{\r\n",
      "\t\t\"class\":{\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t\"stages\" : [\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\"name\": \"Intro\",\r\n",
      "\t\t\t\t\t\"steps\": [\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"type\":\"text\",\t\r\n",
      "\t\t\t\t\t\t\"tag\":\"Landlord\",\r\n",
      "\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <NNP> (<IN> <VBP>)?}\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"type\":\"text\",\t\r\n",
      "\t\t\t\t\t\t\"tag\":\"Tenant\",\r\n",
      "\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <NNP> (<IN> <VBP>)?}\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\t\t\t\t\t{\r\n",
      "\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"Date\",\r\n",
      "\t\t\t\t\t\t\"regex1\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t}\r\n",
      "\r\n",
      "\t\t\t\t\t]\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\"name\": \"Term\",\r\n",
      "\t\t\t\t\t\"steps\":[\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"term_type\": \"Fixed\",\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"beginning on\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"term_type\": \"Fixed\",\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"ending on\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"term_type\": \"Month\",\r\n",
      "\t\t\t\t\t\t\"type\":\"date\",\r\n",
      "\t\t\t\t\t\t\"tag\":\"beginning on\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\d+/\\\\d+/\\\\d+\"\r\n",
      "\t\t\t\t\t}\r\n",
      "\t\t\t\t\t]\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\"name\": \"Rent\",\r\n",
      "\t\t\t\t\t\"steps\":[\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"type\":\"amount\",\r\n",
      "\t\t\t\t\t\t\"tag\": \"Rental amt\",\r\n",
      "\t\t\t\t\t\t\"regex\":\"\\\\$\\\\s+\\\\d+\"\r\n",
      "\t\t\t\t\t}\r\n",
      "\t\t\t\t\t]\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t},\r\n",
      "\r\n",
      "\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\"name\": \"Parties to Contract\",\r\n",
      "\t\t\t\t\t\t\"steps\": [\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\t\"tag\": \"Broker\",\r\n",
      "\t\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <VBZ> <NNP>}\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\t\"tag\": \"Purchaser\",\r\n",
      "\t\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <,> <NN> <VBD> <TO> <IN> <NNP>}\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"type\":\"text\",\r\n",
      "\t\t\t\t\t\t\t\"tag\": \"Seller\",\r\n",
      "\t\t\t\t\t\t\t\"regex\": \"Chunk: {<NNP> <,> <NN> <VBD> <TO> <IN> <NNP>}\"\r\n",
      "\t\t\t\t\t\t}\r\n",
      "\t\t\t\t\t\t]\r\n",
      "\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t}\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\t]\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t\t\r\n",
      "\t\t}\r\n",
      "\t}\r\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_entity = load_string(get_file(ConfigFileName_Entity)).decode('utf-8')\n",
    "print(config_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "\t\"configuration\":{\r\n",
      "\t\t\"classification\":{\r\n",
      "\t\t\t\"stages\":[\r\n",
      "\t\t\t\t{\r\n",
      "\t\t\t\t\t\"doctype\":\"Rental\",\r\n",
      "\t\t\t\t\t\"entities\":[\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"Lease Term\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"lease term\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"Rent\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"Rent\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"Security Deposit\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"Security Deposit\"\r\n",
      "\t\t\t\t\t\t}\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t]\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t{\r\n",
      "\t\t\t\t\t\"doctype\":\"Purchase\",\r\n",
      "\t\t\t\t\t\"entities\":[\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"PARTIES TO CONTRACT - PROPERTY\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"PARTIES TO CONTRACT - PROPERTY\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"EARNEST MONEY DEPOSIT\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"EARNEST MONEY DEPOSIT\"\r\n",
      "\t\t\t\t\t\t},\r\n",
      "\t\t\t\t\t\t{\r\n",
      "\t\t\t\t\t\t\t\"tag\":\"PURCHASE PRICE\",\r\n",
      "\t\t\t\t\t\t\t\"text\":\"PURCHASE PRICE\"\r\n",
      "\t\t\t\t\t\t}\r\n",
      "\t\t\t\t\t\r\n",
      "\t\t\t\t\t]\r\n",
      "\t\t\t\t}\r\n",
      "\t\t\t\t\r\n",
      "\t\t\t]\r\n",
      "\t\t\r\n",
      "\t\t}\r\n",
      "\t\r\n",
      "\t}\r\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config_class = load_string(get_file(ConfigFileName_Classify)).decode('utf-8')\n",
    "print(config_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entity Extraction\n",
    "Extract required entities present in the document and augment the response to NLU's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Entites Extracted by Watson NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_using_NLU(analysistext):\n",
    "    \"\"\" Call Watson Natural Language Understanding service to obtain analysis results.\n",
    "    \"\"\"\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,\n",
    "        features=Features(keywords=KeywordsOptions()))\n",
    "    response = [r['text'] for r in response['keywords']]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Extract Entity-Value \n",
    "Custom entity extraction utlity fucntions for augumenting the results of Watson NLU API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    sent = re.sub(r'\\n',' ',text)\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    POSofText = nltk.tag.pos_tag(words)\n",
    "    return POSofText\n",
    "\n",
    "\n",
    "entval= dict()\n",
    "def text_extract(reg, tag,text):\n",
    "    \"\"\" Use Chunking to extract text from sentence\n",
    "    \"\"\"\n",
    "    entities = list()\n",
    "    chunkParser= nltk.RegexpParser(reg)\n",
    "    chunked= chunkParser.parse(POS_tagging(text))\n",
    "    #print(chunked)\n",
    "    for subtree in chunked.subtrees():\n",
    "        if subtree.label() == 'Chunk':\n",
    "            #print(subtree.leaves())\n",
    "            entities.append(subtree.leaves())\n",
    "    #print(entities)\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(len(entities[i])):\n",
    "            #print(entities[i][j][0].lower())\n",
    "            if tag.strip().lower() in entities[i][j][0].lower():\n",
    "                #print(entities[i])\n",
    "                entval.update({tag: find_NNP(entities[i],tag)})\n",
    "    return entval\n",
    "\n",
    "\n",
    "def find_NNP(ent, tag):\n",
    "    \"\"\" Find NNP POS tags\n",
    "    \"\"\"\n",
    "    e= ent\n",
    "    for i in range(len(e)):\n",
    "        if (tag not in e[i]) and (e[i][1] == 'NNP'):\n",
    "            return e[i][0]\n",
    "\n",
    "\n",
    "\n",
    "def checkValid(date):\n",
    "    #f= datetime.datetime.strftime(date)\n",
    "    try:\n",
    "        datetime.datetime.strptime(date.strip(),\"%d/%m/%Y\")\n",
    "        return 1\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        return 0\n",
    "    \n",
    "def date_extract(reg, tag, text, stage_name):\n",
    "    #print(reg)\n",
    "    d= dict()\n",
    "    dates=re.findall(tag.lower()+' '+reg,text.lower())\n",
    "    print(dates)\n",
    "    temp= dates[0].strip(tag.lower())\n",
    "    ret= checkValid(temp)\n",
    "    if ret == 1:\n",
    "        d.update({tag.lower():temp})\n",
    "    print(d)\n",
    "\n",
    "def amt_extract(reg,tag,text):\n",
    "    a= dict()\n",
    "    amt= re.findall(reg,text)\n",
    "    print(amt)\n",
    "    \n",
    "entities_req= list()\n",
    "def entities_required(text,step, types):\n",
    "    \"\"\" Extracting entities required from configuration file\n",
    "    \"\"\"\n",
    "    configjson= json.loads(config_entity)\n",
    "    for i in range(len(step)):\n",
    "        if step[i]['type'] == types:\n",
    "            entities_req.append(str(step[i]['tag']))\n",
    "            #entities_req.append([c['tag'] for c in configjson['configuration']['class'][i]['steps'][j]])\n",
    "    return entities_req\n",
    "\n",
    "# entlist= list()\n",
    "def extract_entities(config,text):\n",
    "    \"\"\" Extracts entity-value pairs\n",
    "    \"\"\"\n",
    "    configjson= json.loads(config)\n",
    "    #print(configjson)\n",
    "    #print(configjson['configuration']['class'][0]['steps'][0]['entity'][0]['tag'])\n",
    "    classes=configjson['configuration']['class']\n",
    "    #for i in range(len(classes)):\n",
    "    stages= classes['stages']\n",
    "    for j in range(len(stages)):\n",
    "        if stages[j]['name']=='Intro':\n",
    "            steps= stages[j]['steps']\n",
    "            for k in range(len(steps)):\n",
    "                if steps[k]['type'] == 'text':\n",
    "                        #temp=entities_required(text,steps,steps[k]['type'])\n",
    "                            #print(temp)\n",
    "                    ent = text_extract(steps[k]['regex'],steps[k]['tag'],text)\n",
    "                #elif steps[k]['type'] == 'date':\n",
    "                    #dates= date_extract(steps[k]['regex1'],steps[k]['tag'],text, stages[j]['name'])\n",
    "        elif stages[j]['name']=='Parties to Contract':\n",
    "            steps= stages[j]['steps']\n",
    "            for k in range(len(steps)):\n",
    "                if steps[k]['type'] == 'text':\n",
    "                        #temp=entities_required(text,steps,steps[k]['type'])\n",
    "                    ent = text_extract(steps[k]['regex'],steps[k]['tag'],text)\n",
    "                    #print(ent)\n",
    "    \n",
    "    return ent\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tenant': 'W'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extract_entities(config_entity, text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Classification\n",
    "Classify documents based on entities extracted from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_required_classification(text,config):\n",
    "    \"\"\" Extracting entities from configuration file\n",
    "    \"\"\"\n",
    "    entities_req= list()\n",
    "    configjson= json.loads(config)\n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        class_req= stages['doctype']\n",
    "        entities_req.append([[c['text'],class_req] for c in stages['entities']])\n",
    "    return entities_req\n",
    "#entities_required_classification(text2,config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, entities,config):\n",
    "    \"\"\" Classify type of document from list of entities(NLU + Configuration file)\n",
    "    \"\"\"\n",
    "    e= dict()\n",
    "    entities_req= entities_required_classification(text,config)\n",
    "    for i in range(len(entities_req)):\n",
    "        temp= list()\n",
    "        for j in range(len(entities_req[i])):\n",
    "            entities_req[i][j][0]= entities_req[i][j][0].strip()\n",
    "            entities_req[i][j][0]= entities_req[i][j][0].lower()\n",
    "            temp.append(entities_req[i][j][0])\n",
    "            res= analyze_using_NLU(text)\n",
    "            #temp= temp + res\n",
    "            #print text\n",
    "            #text= text.decode('utf-8')\n",
    "        if all(str(x) in text.lower() for x in temp) and any(str(y) in text.lower() for y in res):\n",
    "            return entities_req[i][j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_classify(text,config,config1):\n",
    "    \"\"\" Classify type of Document\n",
    "    \"\"\"\n",
    "    entities= analyze_using_NLU(text)\n",
    "    temp= extract_entities(config,text)\n",
    "    for k,v in temp.items():\n",
    "        entities.append(k)\n",
    "    #print(entities)\n",
    "    entities= [e.lower() for e in entities]\n",
    "    entities= [e.strip() for e in entities]\n",
    "    entities= set(entities)\n",
    "    ret=classify_text(text,entities,config1)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rental'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classify(text_file,config_entity,config_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
